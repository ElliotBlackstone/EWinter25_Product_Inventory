{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f307c4b",
   "metadata": {},
   "source": [
    "# XGBoost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9624e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import root_mean_squared_log_error as rmsle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddc9cc",
   "metadata": {},
   "source": [
    "## Load The Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c418a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data set from kaggle: https://www.kaggle.com/competitions/grupo-bimbo-inventory-demand/data\n",
    "\n",
    "# load train.csv\n",
    "data_path = \"..\\product-inventory\"\n",
    "filename = os.path.join(data_path, \"grupo-bimbo-inventory-demand/train.csv.zip\")\n",
    "\n",
    "train = pd.read_csv(filename, \n",
    "                 usecols=['Semana', 'Producto_ID', 'Cliente_ID', 'Demanda_uni_equil'])\n",
    "\n",
    "# rename columns\n",
    "train = train.rename(columns={  'Semana': 'Week_num',\n",
    "                                'Cliente_ID': 'Client_ID',\n",
    "                                'Demanda_uni_equil': 'adjusted_demand',\n",
    "                                'Producto_ID': 'Product_ID'})\n",
    "\n",
    "# # define client-product ID\n",
    "# train['ID'] = train.groupby(['Client_ID', 'Product_ID']).ngroup()\n",
    "# unique_ids = train['ID'].unique()\n",
    "\n",
    "# # Define the fraction of IDs to sample\n",
    "# fraction = 0.1  # sample 10% of the IDs\n",
    "\n",
    "# # Calculate the number of IDs to sample\n",
    "# sample_size = int(len(unique_ids) * fraction)\n",
    "\n",
    "# rng = np.random.default_rng(4325252122)\n",
    "\n",
    "# # Choose a random sample of IDs\n",
    "# sampled_ids = np.random.choice(unique_ids, size=sample_size, replace=False)\n",
    "\n",
    "# # Filter the DataFrame to keep all rows with the sampled IDs\n",
    "# train = train[train['ID'].isin(sampled_ids)]\n",
    "# train = train.drop(columns='ID')\n",
    "# print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c422c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates client-product-week observation -> take the average as adjusted demand\n",
    "train = train.groupby(['Client_ID', 'Product_ID', 'Week_num'], as_index=False).agg({'adjusted_demand': 'sum'})\n",
    "\n",
    "# create the target dataset and training dataset\n",
    "# identify the last observation of each client-product\n",
    "last_week = train.groupby(['Client_ID', 'Product_ID'], as_index=False).agg({'Week_num':'max'})\n",
    "train  = train.merge(right = last_week,\n",
    "                    how='left',\n",
    "                    on=['Client_ID', 'Product_ID'],\n",
    "                    suffixes=['','_max'])\n",
    "\n",
    "del last_week\n",
    "\n",
    "# target dataset\n",
    "val = train.loc[train['Week_num'] == train['Week_num_max']]\n",
    "val = val.drop(columns=['Week_num_max'])\n",
    "\n",
    "# train dataset\n",
    "train = train.loc[train['Week_num'] != train['Week_num_max']]\n",
    "train = train.drop(columns=['Week_num_max'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d21a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train), len(val))\n",
    "print('\\n')\n",
    "print(train.columns)\n",
    "print(val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf348ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of aggregate statistics for each client\n",
    "client_stats = train.groupby(['Client_ID'], as_index=False).agg({'Product_ID':'nunique', 'adjusted_demand':['mean', 'median']})\n",
    "client_stats.columns = ['Client_ID', 'Products', 'adj_dem_mean', 'adj_dem_median']\n",
    "client_stats['adj_dem_mean'] = client_stats['adj_dem_mean'].round(2)\n",
    "client_stats['adj_dem_median'] = client_stats['adj_dem_median'].astype(int)\n",
    "\n",
    "#create a dataframe of aggregate statistics for each product\n",
    "product_stats = train.groupby(['Product_ID'], as_index=False).agg({'Client_ID':'nunique', 'adjusted_demand':['mean', 'median']})\n",
    "product_stats.columns = ['Product_ID', 'Clients', 'adj_dem_mean', 'adj_dem_median']\n",
    "product_stats['adj_dem_mean'] = product_stats['adj_dem_mean'].round(2)\n",
    "product_stats['adj_dem_median'] = product_stats['adj_dem_median'].astype(int)\n",
    "product_stats['median_pct'] = product_stats['adj_dem_median'].rank(pct=True, method='average')\n",
    "\n",
    "#create a dataframe of aggregate client-product information i.e. removing the time dimension\n",
    "train = train.groupby(['Client_ID', 'Product_ID'], as_index=False).agg({'Week_num':'nunique', 'adjusted_demand':['mean', 'median', 'min', 'max']})\n",
    "train.columns = ['Client_ID', 'Product_ID', 'num_weeks', 'adj_dem_mean', 'adj_dem_median', 'adj_dem_min', 'adj_dem_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd4ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(right=client_stats,\n",
    "                    how='left',\n",
    "                    on='Client_ID',\n",
    "                    suffixes=('', '_c'))\n",
    "del client_stats\n",
    "\n",
    "train = train.merge(right=product_stats,\n",
    "                    how='left',\n",
    "                    on='Product_ID',\n",
    "                    suffixes=('','_p'))\n",
    "del product_stats\n",
    "\n",
    "train = train.merge(right=val[['Client_ID', 'Product_ID', 'adjusted_demand']],\n",
    "                how='left',\n",
    "                on=['Product_ID', 'Client_ID'])\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['num_weeks','adj_dem_median', 'adj_dem_min', 'adj_dem_max',\n",
    "            'Products','adj_dem_mean_c', 'adj_dem_median_c',\n",
    "            'Clients', 'adj_dem_mean_p', 'adj_dem_median_p', 'median_pct']\n",
    "target = ['adjusted_demand']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66818073",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"max_depth\":    [3, 4, 5],\n",
    "              \"n_estimators\": np.arange(100,800,100),\n",
    "              \"learning_rate\": [0.01, 0.1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:  this took about 3 minutes to run on my 2023 MacBook Pro.\n",
    "search = GridSearchCV(xgb_reg, param_grid, cv=5, scoring = 'neg_root_mean_squared_log_error').fit(train[features], train[target])\n",
    "\n",
    "print(\"The best hyperparameters are \",search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=False)\n",
    "bst = xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the demand\n",
    "train['pred'] = xgb_reg.predict(X_train)\n",
    "train['pred'] = np.maximum(0, train['pred']).round(2)\n",
    "train[target+['pred', 'Client_ID', 'Product_ID']+features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4cb14",
   "metadata": {},
   "source": [
    "## Load The Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test.csv\n",
    "data_path = \"..\\product-inventory\"\n",
    "filename = os.path.join(data_path, \"grupo-bimbo-inventory-demand/test.csv.zip\")\n",
    "\n",
    "test = pd.read_csv(filename, \n",
    "                 usecols=['id', 'Producto_ID', 'Cliente_ID'])\n",
    "# \n",
    "# rename columns\n",
    "test = test.rename(columns={'Cliente_ID': 'Client_ID',\n",
    "                            'Producto_ID': 'Product_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['id','Client_ID', 'Product_ID']].merge(right=train[['Client_ID', 'Product_ID', 'pred']], \n",
    "                                                    how='left', \n",
    "                                                    on=['Client_ID', 'Product_ID'])\n",
    "test = test.sort_values(by=['Client_ID', 'Product_ID']).reset_index(drop=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred'].isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40bc03",
   "metadata": {},
   "source": [
    "## Out-of-Sample Prediction\n",
    "There are a few cases where the client-product combo is not present in the training data. I fill in these observations' missing values. \n",
    "### 1. Existing Products\n",
    "This case consists of two possible scanarios:\n",
    "* New Client\n",
    "* Existing Client but a new combo\n",
    "\n",
    "In both scenarios, the in-sample estimation can not predict the demand. We use the average product demand in weeks 3-9 as our prediction.\n",
    "In the 2nd scenario, our prediction model has some shortcomings. For example, our measure does not take into account that a client might have a low demand for a new product. On the other hand, if we use the client's average demand as our prediction, it does not take into account the variation in products' demand. As a first pass, we use 'average prodct demand' as the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f167f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use product's average demand in week 3-9 as a prediction for the new client.\n",
    "# adj_dem_mean_p contains existing product with new client, and a prediction for the client's demand.\n",
    "\n",
    "#create a dataframe of aggregate statistics for each product\n",
    "testagg = train.groupby(['Product_ID'], as_index=False).agg({'adj_dem_mean_p':'mean'})\n",
    "\n",
    "test = test.merge(right=testagg, \n",
    "                  how='left', \n",
    "                  on='Product_ID')\n",
    "\n",
    "del testagg\n",
    "test['pred'] = test['pred'].fillna(test['adj_dem_mean_p'])\n",
    "print('Share of Missing Preiction:', test['pred'].isna().mean())\n",
    "\n",
    "test = test.drop(columns='adj_dem_mean_p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e69065",
   "metadata": {},
   "source": [
    "### 2. Existing Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3efeed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use client's average demand in week 3-9 as a prediction for new product.\n",
    "#pred_1 contains existing clients with new products, and a prediction for the product.\n",
    "\n",
    "#create a dataframe of aggregate statistics for each product\n",
    "testagg = train.groupby(['Client_ID'], as_index=False).agg({'adj_dem_mean_c':'mean'})\n",
    "test = test.merge(right=testagg, \n",
    "                  how='left', \n",
    "                  on='Client_ID')\n",
    "\n",
    "test['pred'] = test['pred'].fillna(test['adj_dem_mean_c'])\n",
    "print('Share of Missing Preiction:', test['pred'].isna().mean())\n",
    "\n",
    "del testagg\n",
    "test = test.drop(columns='adj_dem_mean_c')\n",
    "# WATCH OUT: This replaces missing values for existing clients and existing products with the client's average demand. \n",
    "# THEY SHOULD BE REPLACED WITH THE ACTUAL PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538572ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72360c",
   "metadata": {},
   "source": [
    "### 3. New Clients, New Products\n",
    "The intersection of new clients and new product in the test data. Here the first guess is the average demand for all product across all weeks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred'] = test['pred'].fillna(train['adj_dem_mean'].mean())\n",
    "print('Share of Missing Preiction:', test['pred'].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce878eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc70748",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = test[['id', 'pred']]\n",
    "output = output.rename(columns={'pred': 'Demanda_uni_equil'})\n",
    "\n",
    "data_path = \"..\\product-inventory\"\n",
    "filename = os.path.join(data_path, \"prediction_3.csv\")\n",
    "output.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_spring_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
